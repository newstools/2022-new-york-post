The feds have escalated a probe into crashes in which Tesla vehicles using the semiautonomous “Autopilot” system failed to detect emergency vehicles in a potential precursor to a car recall. The National Highway Traffic Safety Administration said Thursday it was advancing its ongoing probe into the crashes to an engineering analysis – the final step required before federal officials decide whether to enact a recall. The feds say they have now identified a total of 16 crashes involving Tesla vehicles with “Autopilot” enabled and parked emergency vehicles or trucks. Those incidents purportedly resulted in 15 injuries and one death. The engineering review will “extend the existing crash analysis, evaluate additional data sets, perform vehicle evaluations, and to explore the degree to which Autopilot and associated Tesla systems may exacerbate human factors or behavioral safety risks by undermining the effectiveness of the driver’s supervision,” according to the NHTSA’s notice. The expanded probe applies to approximately 830,000 Tesla vehicles with Autopilot functionality. That amounts to nearly all Tesla cars sold in the US since the 2014 model year, according to the Associated Press. Federal safety officials said their analysis determined Autopilot’s “Forward Collision Warnings (FCW) activated in the majority of incidents immediately prior to impact and that subsequent Automatic Emergency Braking (AEB) intervened in approximately half of the collisions.” “On average in these crashes, Autopilot aborted vehicle control less than one second prior to the first impact,” the NHTSA’s notice said. Efforts to reach Tesla representatives were not immediately successful. CEO Elon Musk purportedly scrapped the company’s media relations team in 2020. Tesla shares were flat in trading Thursday. The NHTSA first launched its probe of Tesla’s Autopilot system last August. The initial scope of the investigation focused on 12 crashes involving Tesla cars and emergency vehicles. Musk has long asserted that Autopilot is an effective system that improves safety conditions on the road rather than posing a hazard to motorists. The system automates some elements of driving by providing lane-keeping assistance and adaptive cruise control but still requires human oversight to function. “Essentially, passive Autopilot (car intervenes only when crash probability is high) cuts crashes in half,” Musk said in an April 2021 tweet. “Active Autopilot (car is driving itself) cuts crashes in half again. Doesn’t mean there are no crashes, but, on balance, Autopilot is unequivocally safer.” Democratic Sen. Ed Markey of Massachusetts lauded the agency’s decision to advance the probe. “Every day that Tesla disregards safety rules and misleads the public about its ‘Autopilot” system, our roads become more dangerous,” Markey tweeted. Musk has repeatedly clashed with federal safety regulators over their scrutiny of Tesla vehicles and software. In February, Musk called federal regulators the “fun police” after the company issued a recall over a “Beatbox” program that allowed drivers to replace their car horns with custom sounds, including fart noises. Safety officials feared the feature could drown out the sound of a pedestrian warning system.